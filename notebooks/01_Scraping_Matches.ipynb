{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a6b758-ba69-491f-a104-eadb2900326e",
   "metadata": {},
   "source": [
    "<center><h1><font size=6> Scraping Historic EPL Match Data </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bfb65-9561-40ea-aee3-fd910924c0ab",
   "metadata": {},
   "source": [
    "This notebook scrapes data on the results and match statistics of historic Premier League football matches from [this EPL data page](https://fbref.com/en/comps/9/Premier-League-Stats). The data is collected from the 2017-18 season onwards as I want to make use of the Expected Goals statistic, which was only introduced for the EPL in the 2017-18 season."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c821a-f7c9-438b-b963-91c69e4c490c",
   "metadata": {},
   "source": [
    "### Load libraries and setup notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8645266-551c-46b1-9beb-54c4b57bcd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "# set pandas configurations\n",
    "pd.set_option(\"display.precision\", 2) # display to 1 decimpal place\n",
    "pd.set_option(\"display.max.columns\", None) # display all columns so we can view the whole dataset\n",
    "\n",
    "\n",
    "# set directories\n",
    "os.chdir('..') # change current working directory to the parent directory to help access files/directories at a higher level\n",
    "DATAPATH = Path(r'data') # set data path\n",
    "\n",
    "\n",
    "# import from source directory\n",
    "from src import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c635c50-8bdf-4b1c-a43d-11b0fa17828b",
   "metadata": {},
   "source": [
    "### Defining inputs to the scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390707c9-486b-41e8-824e-f11f76a83c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2022, 2021, 2020, 2019, 2018, 2017]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty object to store the data\n",
    "all_matches = []\n",
    "\n",
    "\n",
    "# create a today's date object to keep a record of when data was downloaded\n",
    "todays_date = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# define URL link for the latest season\n",
    "season_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\" # define URL for the season's page\n",
    "\n",
    "\n",
    "# creates a list of years (seasons) to repeat the data scraping loop over\n",
    "years = list(range(constants.LATEST_SEASON, 2016, -1)) # we only include data post-2017 because crucual predictors like expected goals are not available before then\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7fa0a-30b3-4c3a-9ec8-4c7def16d786",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "\n",
    "The below code runs a loop to go through each season (as specified above) and:\n",
    "1. Collect the names and URL links of all the teams that played in the EPL in that season.\n",
    "2. From each team's link, scrape their basic match data including result, goals, expected goals, possession, and other bits of info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77c2b1c-915f-49ac-94df-905ea5f8a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for Manchester City in year 2022 successfully colected.\n",
      "Data for Arsenal in year 2022 successfully colected.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m         all_matches\u001b[38;5;241m.\u001b[39mappend(matches) \u001b[38;5;66;03m# append team data to dataframe\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mteam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully colected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# rest before moving on\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# exit the while loop if the scraping is successful\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    \n",
    "    # wrap code in a while loop that keeps trying until the scraping is successful\n",
    "    while True:\n",
    "        try:\n",
    "    \n",
    "            # collect the website links for each squad in the premier league for the latest season as defined above\n",
    "            season_response = requests.get(season_url) # send GET request to URL and store response\n",
    "            season_request_text = BeautifulSoup(season_response.text) # get the text content by parsing the HTML response\n",
    "            season_standings_table = season_request_text.select('table.stats_table')[0] # collect the league table from the text content\n",
    "            season_all_links =  [l.get(\"href\") for l in season_standings_table.find_all('a')] # find all links from the table and extract href objects\n",
    "            season_team_links = [l for l in season_all_links if '/squads/' in l] # collect all links that contain '/squads' in their URLs\n",
    "            season_team_urls = [f\"https://fbref.com{l}\" for l in season_team_links] # complete the URLs by adding website opening text on the front\n",
    "\n",
    "\n",
    "            # collect the URL for the previous season and set it to the new season URL to set up next stage of the loop\n",
    "            previous_season_href = season_request_text.select(\"a.prev\")[0].get(\"href\") # collect the href link for the previous season\n",
    "            season_url = f\"https://fbref.com{previous_season_href}\" # complete the previous season url to set up next stage of the loop once data on all teams are collected\n",
    "\n",
    "\n",
    "            # for each URL in the collected team URLs, scrape match data\n",
    "            for team_url in season_team_urls:\n",
    "\n",
    "                team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") # obtain the team name from the URL\n",
    "                team_df_list = pd.read_html(team_url) # collect list of tables from team web page\n",
    "                matches = pd.read_html(team_url)[1] # collect the 2nd table which contains the match data\n",
    "                matches[\"season\"] = year # create a new column to add the season\n",
    "                matches[\"team\"] = team_name # create a new column to add the team\n",
    "                matches[\"date_donwloaded\"] = todays_date # create a new column to keep track of when data was downloaded\n",
    "\n",
    "                all_matches.append(matches) # append team data to dataframe\n",
    "\n",
    "                print(f\"Data for {team_name} in year {year} successfully colected.\")\n",
    "                \n",
    "                time.sleep(5) # rest before moving on\n",
    "            \n",
    "            break # exit the while loop if the scraping is successful\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "            print(\"Retrying after 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "        \n",
    "\n",
    "matches_df = pd.concat(all_matches) # concatinate into a dataframe\n",
    "matches_df.to_csv(f\"{DATAPATH}/raw/matches_raw.csv\") # store this raw data as csv in the local data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884eabae-33f5-4d92-97ef-eec99d90ca9f",
   "metadata": {},
   "source": [
    "### Repeat another scraping process that scrapes more historic data (but not xG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169dcb4f-6012-49ba-95df-a3a71f309661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty object to store the data\n",
    "all_matches_long = []\n",
    "\n",
    "\n",
    "# define URL link for the latest season\n",
    "season_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\" # define URL for the season's page\n",
    "\n",
    "\n",
    "# creates a list of years (seasons) to repeat the data scraping loop over\n",
    "years = list(range(constants.LATEST_SEASON, 1991, -1)) # we only include data post-2017 because crucual predictors like expected goals are not available before then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae8754-8481-49dd-b814-9a7a495741eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    \n",
    "    # wrap code in a while loop that keeps trying until the scraping is successful\n",
    "    while True:\n",
    "        try:\n",
    "    \n",
    "            # collect the website links for each squad in the premier league for the latest season as defined above\n",
    "            season_response = requests.get(season_url) # send GET request to URL and store response\n",
    "            season_request_text = BeautifulSoup(season_response.text) # get the text content by parsing the HTML response\n",
    "            season_standings_table = season_request_text.select('table.stats_table')[0] # collect the league table from the text content\n",
    "            season_all_links =  [l.get(\"href\") for l in season_standings_table.find_all('a')] # find all links from the table and extract href objects\n",
    "            season_team_links = [l for l in season_all_links if '/squads/' in l] # collect all links that contain '/squads' in their URLs\n",
    "            season_team_urls = [f\"https://fbref.com{l}\" for l in season_team_links] # complete the URLs by adding website opening text on the front\n",
    "\n",
    "\n",
    "            # collect the URL for the previous season and set it to the new season URL to set up next stage of the loop\n",
    "            previous_season_href = season_request_text.select(\"a.prev\")[0].get(\"href\") # collect the href link for the previous season\n",
    "            season_url = f\"https://fbref.com{previous_season_href}\" # complete the previous season url to set up next stage of the loop once data on all teams are collected\n",
    "\n",
    "\n",
    "            # for each URL in the collected team URLs, scrape match data\n",
    "            for team_url in season_team_urls:\n",
    "\n",
    "                team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") # obtain the team name from the URL\n",
    "                team_df_list = pd.read_html(team_url) # collect list of tables from team web page\n",
    "                matches = pd.read_html(team_url)[1] # collect the 2nd table which contains the match data\n",
    "                matches[\"season\"] = year # create a new column to add the season\n",
    "                matches[\"team\"] = team_name # create a new column to add the team\n",
    "                matches[\"date_donwloaded\"] = todays_date # create a new column to keep track of when data was downloaded\n",
    "\n",
    "                all_matches_long.append(matches) # append team data to dataframe\n",
    "\n",
    "                print(f\"Data for {team_name} in year {year} successfully colected.\")\n",
    "                \n",
    "                time.sleep(5) # rest before moving on\n",
    "            \n",
    "            break # exit the while loop if the scraping is successful\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "            print(\"Retrying after 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "        \n",
    "\n",
    "matches_df_long = pd.concat(all_matches_long) # concatinate into a dataframe\n",
    "matches_df_long.to_csv(f\"{DATAPATH}/raw/matches_long_raw.csv\") # store this raw data as csv in the local data file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
